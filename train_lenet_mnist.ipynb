{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241b7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from biotorch.initialization.functions import convert_module\n",
    "from biotorch.layers import Linear, Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6036da",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cba9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic LeNet Architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation='tanh'):\n",
    "        \"\"\"\n",
    "        :param in_features: dimension of input features (784 for MNIST)\n",
    "        :param num_layers: number of layers for feed-forward net\n",
    "        :param num_hidden_list: list of integers indicating hidden nodes of each layer\n",
    "        \"\"\"\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "\n",
    "        # create layer operations\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        forward pass, which is same for conventional feed-forward net\n",
    "        :param inputs: inputs with shape [batch_size, in_features]\n",
    "        :return: logit outputs from the network\n",
    "        \"\"\"\n",
    "        inputs = self.activation(self.conv1(inputs))\n",
    "        inputs = self.pool(inputs)\n",
    "\n",
    "        inputs = self.activation(self.conv2(inputs))\n",
    "        inputs = self.pool(inputs)\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0], -1)\n",
    "\n",
    "        inputs = self.activation(self.fc1(inputs))\n",
    "        inputs = self.fc2(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34202f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetFA(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic LeNet Architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation='tanh'):\n",
    "        \"\"\"\n",
    "        :param in_features: dimension of input features (784 for MNIST)\n",
    "        :param num_layers: number of layers for feed-forward net\n",
    "        :param num_hidden_list: list of integers indicating hidden nodes of each layer\n",
    "        \"\"\"\n",
    "        super(LeNetFA, self).__init__()\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "\n",
    "        # create layer operations\n",
    "        self.conv1 = Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = Linear(500, 10)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        forward pass, which is same for conventional feed-forward net\n",
    "        :param inputs: inputs with shape [batch_size, in_features]\n",
    "        :return: logit outputs from the network\n",
    "        \"\"\"\n",
    "        inputs = self.activation(self.conv1(inputs))\n",
    "        inputs = self.pool(inputs)\n",
    "\n",
    "        inputs = self.activation(self.conv2(inputs))\n",
    "        inputs = self.pool(inputs)\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0], -1)\n",
    "\n",
    "        inputs = self.activation(self.fc1(inputs))\n",
    "        inputs = self.fc2(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e04327",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ceceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, batch_size, device):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # Desactivate the autograd engine in test\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #data = data.view(batch_size, -1)\n",
    "            inputs, targets = data.to(device), target.to(device)\n",
    "            predictions = model(inputs)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            test_loss += F.nll_loss(predictions, targets, size_average=False).item()\n",
    "            pred = predictions.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7f74d",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841ed4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba340091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# set up datasets\n",
    "print('==> Preparing data..')\n",
    "\n",
    "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True,\n",
    "                                             transform=transforms.Compose([\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                             ])),\n",
    "                              batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(datasets.MNIST('./data', train=False, download=True,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                            ])),\n",
    "                             batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57898737",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12360e96",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806ca764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Back Propagation Model\n",
    "model_bp = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1319e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feedback Alignment model\n",
    "model_fa = LeNet()\n",
    "# Uncomment for multiple GPUs\n",
    "# model_fa = nn.DataParallel(model_fa, device_ids=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf03330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can comment this to run on GPU\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059b233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the 2 <class 'torch.nn.modules.conv.Conv2d'> layers were converted successfully\n",
      "All the 2 <class 'torch.nn.modules.linear.Linear'> layers were converted successfully\n"
     ]
    }
   ],
   "source": [
    "convert_module(model_fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d0b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.LeNet'>\n",
      "<class 'biotorch.layers.conv.Conv2d'>\n",
      "<class 'biotorch.layers.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'biotorch.layers.linear.Linear'>\n",
      "<class 'biotorch.layers.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model_fa.modules()):\n",
    "    print(type(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f5f145b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95103b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fa.to(device)\n",
    "model_bp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3894a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizers\n",
    "loss_crossentropy = torch.nn.CrossEntropyLoss()\n",
    "optimizer_fa = torch.optim.RMSprop(model_fa.parameters(), lr=1e-4, weight_decay=0.)\n",
    "optimizer_bp = torch.optim.RMSprop(model_bp.parameters(), lr=1e-4, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ca8ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_train = open('results' + 'bp_vs_fa.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a318f5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e7035a9e9282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mt_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmodel_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mt_avg_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_fa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/biotorch/.venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/biotorch/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/projects/biotorch/.venv/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/biotorch/biotorch/autograd/functions.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(context, grad_output)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_fa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_weight_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_bias_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdb' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for idx_batch, (inputs, targets) in enumerate(train_loader):\n",
    "        # flatten the inputs from square image to 1d vector\n",
    "        #inputs = inputs.view(batch_size, -1)\n",
    "        # wrap them into varaibles\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # get outputs from the model\n",
    "        #print(\"inputs = \", inputs.size())\n",
    "        outputs_fa = model_fa(inputs)\n",
    "        outputs_bp = model_bp(inputs)\n",
    "        # print(outputs_fa.size())\n",
    "        # print(outputs_bp.size())\n",
    "        # calculate loss\n",
    "        outputs_fa = torch.squeeze(outputs_fa)\n",
    "        outputs_bp = torch.squeeze(outputs_bp)\n",
    "        # print(outputs_fa.size())\n",
    "        # print(outputs_bp.size())\n",
    "\n",
    "        # print(\"-\"*20)\n",
    "        #print(\"targets.size() = \", targets.size())\n",
    "        # input()\n",
    "        \n",
    "        loss_bp = loss_crossentropy(outputs_bp, targets)\n",
    "        loss_fa = loss_crossentropy(outputs_fa, targets)\n",
    "        # print(loss_bp, loss_fa)\n",
    "        \n",
    "        t_bp = time.time()\n",
    "        model_bp.zero_grad()\n",
    "        loss_bp.backward()\n",
    "        optimizer_bp.step()\n",
    "        t_avg_bp = time.time() - t_bp\n",
    "        \n",
    "        t_fa = time.time()\n",
    "        model_fa.zero_grad()\n",
    "        loss_fa.backward()\n",
    "        optimizer_fa.step()\n",
    "        t_avg_fa = time.time() - t_fa\n",
    "\n",
    "        if (idx_batch + 1) % 100 == 0:\n",
    "            train_log = 'epoch ' + str(epoch) + ' step ' + str(idx_batch + 1) + \\\n",
    "                        ' loss_fa ' + str(loss_fa.data.item()) + ' loss_bp ' + str(loss_bp.data.item())\n",
    "                         \n",
    "            times = ' time_fa '+ str(t_avg_fa) + ' time_bp ' + str(t_avg_bp)\n",
    "            time_dif = t_avg_fa - t_avg_bp\n",
    "            print(train_log)\n",
    "            print(times)\n",
    "            print(time_dif)\n",
    "            logger_train.write(train_log + '\\n')\n",
    "\n",
    "    # Test models\n",
    "    test_loss_fa, correct_fa = test(model_fa, test_loader, batch_size, device)    \n",
    "    test_loss_bp, correct_bp = test(model_bp, test_loader, batch_size, device)\n",
    "\n",
    "    print('\\n[Epoch {}] Test results'.format(epoch))\n",
    "    print('\\tFA: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss_fa,\n",
    "                                                                      correct_fa, len(test_loader.dataset), 100. * correct_fa / len(test_loader.dataset)))\n",
    "    print('\\tBP: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss_bp,\n",
    "                                                                        correct_bp, len(test_loader.dataset), 100. * correct_bp / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccd003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a9feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa76c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biotorch",
   "language": "python",
   "name": "biotorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
