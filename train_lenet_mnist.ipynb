{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optical-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from biotorch.initialization.functions import add_fa_weight_matrices, override_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5e8bf",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c818f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic LeNet Architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation='tanh'):\n",
    "        \"\"\"\n",
    "        :param in_features: dimension of input features (784 for MNIST)\n",
    "        :param num_layers: number of layers for feed-forward net\n",
    "        :param num_hidden_list: list of integers indicating hidden nodes of each layer\n",
    "        \"\"\"\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "\n",
    "        # create layer operations\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        forward pass, which is same for conventional feed-forward net\n",
    "        :param inputs: inputs with shape [batch_size, in_features]\n",
    "        :return: logit outputs from the network\n",
    "        \"\"\"\n",
    "        inputs = self.activation(self.conv1(inputs))\n",
    "        inputs = self.pool(inputs)\n",
    "\n",
    "        inputs = self.activation(self.conv2(inputs))\n",
    "        inputs = self.pool(inputs)\n",
    "\n",
    "        inputs = inputs.view(inputs.size()[0], -1)\n",
    "\n",
    "        inputs = self.activation(self.fc1(inputs))\n",
    "        inputs = self.fc2(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8bd9c",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colored-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, batch_size):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # Desactivate the autograd engine in test\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            #data = data.view(batch_size, -1)\n",
    "            inputs, targets = Variable(data), Variable(target)\n",
    "            predictions = model(inputs)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            test_loss += F.nll_loss(predictions, targets, size_average=False).item()\n",
    "            pred = predictions.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5bcc0d",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8cc5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developmental-stanley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99.1%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/makrout/Documents/PhD/collaboration/albert/biotorch/.venv/lib/python3.6/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# set up datasets\n",
    "print('==> Preparing data..')\n",
    "\n",
    "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True,\n",
    "                                             transform=transforms.Compose([\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                             ])),\n",
    "                              batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(datasets.MNIST('./data', train=False, download=True,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                            ])),\n",
    "                             batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b9630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "standard-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Back Propagation Model\n",
    "model_bp = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae27ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Feedback Alignment model\n",
    "model_fa = LeNet()\n",
    "model_fa.apply(add_fa_weight_matrices)\n",
    "model_fa.apply(override_backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "banned-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizers\n",
    "loss_crossentropy = torch.nn.CrossEntropyLoss()\n",
    "optimizer_fa = torch.optim.RMSprop(model_fa.parameters(), lr=1e-4, weight_decay=0.)\n",
    "optimizer_bp = torch.optim.RMSprop(model_bp.parameters(), lr=1e-4, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facial-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_train = open('results' + 'bp_vs_fa.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fundamental-strain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch 0 step 100 loss_fa 2.28066349029541 loss_bp 0.4225970506668091\n",
      " time_fa 0.005018711090087891 time_bp 0.003319263458251953\n",
      "0.0016994476318359375\n",
      "epoch 0 step 200 loss_fa 2.3583970069885254 loss_bp 0.16006958484649658\n",
      " time_fa 0.009884834289550781 time_bp 0.0029969215393066406\n",
      "0.006887912750244141\n",
      "epoch 0 step 300 loss_fa 2.0788614749908447 loss_bp 0.21277275681495667\n",
      " time_fa 0.005431652069091797 time_bp 0.0032176971435546875\n",
      "0.0022139549255371094\n",
      "epoch 0 step 400 loss_fa 1.7037607431411743 loss_bp 0.23728452622890472\n",
      " time_fa 0.005675792694091797 time_bp 0.0030775070190429688\n",
      "0.002598285675048828\n",
      "epoch 0 step 500 loss_fa 1.2447919845581055 loss_bp 0.30772966146469116\n",
      " time_fa 0.005147695541381836 time_bp 0.0031185150146484375\n",
      "0.0020291805267333984\n",
      "epoch 0 step 600 loss_fa 1.0628862380981445 loss_bp 0.347728967666626\n",
      " time_fa 0.004655122756958008 time_bp 0.003999233245849609\n",
      "0.0006558895111083984\n",
      "epoch 0 step 700 loss_fa 0.4980400502681732 loss_bp 0.0854148119688034\n",
      " time_fa 0.011282682418823242 time_bp 0.0029528141021728516\n",
      "0.00832986831665039\n",
      "epoch 0 step 800 loss_fa 0.46501338481903076 loss_bp 0.11358568072319031\n",
      " time_fa 0.005153656005859375 time_bp 0.004291057586669922\n",
      "0.0008625984191894531\n",
      "epoch 0 step 900 loss_fa 0.6831474900245667 loss_bp 0.17216986417770386\n",
      " time_fa 0.005357265472412109 time_bp 0.003172636032104492\n",
      "0.002184629440307617\n",
      "epoch 0 step 1000 loss_fa 0.2737942636013031 loss_bp 0.041481178253889084\n",
      " time_fa 0.005115032196044922 time_bp 0.0031478404998779297\n",
      "0.001967191696166992\n",
      "epoch 0 step 1100 loss_fa 0.4876024127006531 loss_bp 0.07931311428546906\n",
      " time_fa 0.005138874053955078 time_bp 0.0033538341522216797\n",
      "0.0017850399017333984\n",
      "epoch 0 step 1200 loss_fa 0.34268441796302795 loss_bp 0.026766983792185783\n",
      " time_fa 0.01176595687866211 time_bp 0.002973794937133789\n",
      "0.00879216194152832\n",
      "epoch 0 step 1300 loss_fa 0.4211065471172333 loss_bp 0.07254590839147568\n",
      " time_fa 0.004689931869506836 time_bp 0.003434896469116211\n",
      "0.001255035400390625\n",
      "epoch 0 step 1400 loss_fa 0.31739306449890137 loss_bp 0.053759388625621796\n",
      " time_fa 0.011452198028564453 time_bp 0.0032503604888916016\n",
      "0.008201837539672852\n",
      "epoch 0 step 1500 loss_fa 0.3009219765663147 loss_bp 0.07498764991760254\n",
      " time_fa 0.010470151901245117 time_bp 0.0032896995544433594\n",
      "0.007180452346801758\n",
      "epoch 0 step 1600 loss_fa 0.26551347970962524 loss_bp 0.10291644185781479\n",
      " time_fa 0.005883216857910156 time_bp 0.0034563541412353516\n",
      "0.0024268627166748047\n",
      "epoch 0 step 1700 loss_fa 0.5812040567398071 loss_bp 0.12764589488506317\n",
      " time_fa 0.005098819732666016 time_bp 0.003324747085571289\n",
      "0.0017740726470947266\n",
      "epoch 0 step 1800 loss_fa 0.412973016500473 loss_bp 0.15131287276744843\n",
      " time_fa 0.01127171516418457 time_bp 0.0030100345611572266\n",
      "0.008261680603027344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albert/projects/biotorch/.venv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 0] Test results\n",
      "\tFA: Avg. loss: -7.5135, Accuracy: 9190/10000 (92%)\n",
      "\tBP: Avg. loss: -8.7437, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "epoch 1 step 100 loss_fa 0.540947437286377 loss_bp 0.14071831107139587\n",
      " time_fa 0.004911899566650391 time_bp 0.0032815933227539062\n",
      "0.0016303062438964844\n",
      "epoch 1 step 200 loss_fa 0.3962908685207367 loss_bp 0.048438020050525665\n",
      " time_fa 0.004953622817993164 time_bp 0.003183603286743164\n",
      "0.00177001953125\n",
      "epoch 1 step 300 loss_fa 0.15675963461399078 loss_bp 0.014619681984186172\n",
      " time_fa 0.005177021026611328 time_bp 0.003385305404663086\n",
      "0.0017917156219482422\n",
      "epoch 1 step 400 loss_fa 0.17005963623523712 loss_bp 0.03276843577623367\n",
      " time_fa 0.012292146682739258 time_bp 0.003499269485473633\n",
      "0.008792877197265625\n",
      "epoch 1 step 500 loss_fa 0.24827104806900024 loss_bp 0.06215215101838112\n",
      " time_fa 0.005931377410888672 time_bp 0.0033042430877685547\n",
      "0.002627134323120117\n",
      "epoch 1 step 600 loss_fa 0.24202530086040497 loss_bp 0.05218551680445671\n",
      " time_fa 0.0063593387603759766 time_bp 0.002958536148071289\n",
      "0.0034008026123046875\n",
      "epoch 1 step 700 loss_fa 0.1781044751405716 loss_bp 0.05104460567235947\n",
      " time_fa 0.005444765090942383 time_bp 0.003418445587158203\n",
      "0.0020263195037841797\n",
      "epoch 1 step 800 loss_fa 0.1367427110671997 loss_bp 0.007979543879628181\n",
      " time_fa 0.01005864143371582 time_bp 0.004002809524536133\n",
      "0.0060558319091796875\n",
      "epoch 1 step 900 loss_fa 0.12157368659973145 loss_bp 0.03793667256832123\n",
      " time_fa 0.0047016143798828125 time_bp 0.003433704376220703\n",
      "0.0012679100036621094\n",
      "epoch 1 step 1000 loss_fa 0.2771863043308258 loss_bp 0.04290282353758812\n",
      " time_fa 0.004948616027832031 time_bp 0.0031723976135253906\n",
      "0.0017762184143066406\n",
      "epoch 1 step 1100 loss_fa 0.17933227121829987 loss_bp 0.05939760059118271\n",
      " time_fa 0.00529170036315918 time_bp 0.004177093505859375\n",
      "0.0011146068572998047\n",
      "epoch 1 step 1200 loss_fa 0.09249213337898254 loss_bp 0.019180672243237495\n",
      " time_fa 0.005057096481323242 time_bp 0.0033359527587890625\n",
      "0.0017211437225341797\n",
      "epoch 1 step 1300 loss_fa 0.3559262454509735 loss_bp 0.18285909295082092\n",
      " time_fa 0.009660005569458008 time_bp 0.003233671188354492\n",
      "0.006426334381103516\n",
      "epoch 1 step 1400 loss_fa 0.4176253080368042 loss_bp 0.10293426364660263\n",
      " time_fa 0.00491023063659668 time_bp 0.0030231475830078125\n",
      "0.0018870830535888672\n",
      "epoch 1 step 1500 loss_fa 0.17583708465099335 loss_bp 0.026325182989239693\n",
      " time_fa 0.005214691162109375 time_bp 0.0033998489379882812\n",
      "0.0018148422241210938\n",
      "epoch 1 step 1600 loss_fa 0.046367764472961426 loss_bp 0.01650025136768818\n",
      " time_fa 0.0049169063568115234 time_bp 0.0030183792114257812\n",
      "0.0018985271453857422\n",
      "epoch 1 step 1700 loss_fa 0.04857850447297096 loss_bp 0.0244891420006752\n",
      " time_fa 0.0051267147064208984 time_bp 0.0033311843872070312\n",
      "0.0017955303192138672\n",
      "epoch 1 step 1800 loss_fa 0.29263240098953247 loss_bp 0.05271438881754875\n",
      " time_fa 0.005259990692138672 time_bp 0.0032835006713867188\n",
      "0.001976490020751953\n",
      "\n",
      "[Epoch 1] Test results\n",
      "\tFA: Avg. loss: -8.6394, Accuracy: 9474/10000 (95%)\n",
      "\tBP: Avg. loss: -9.9932, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "epoch 2 step 100 loss_fa 0.16677497327327728 loss_bp 0.08891596645116806\n",
      " time_fa 0.00607752799987793 time_bp 0.0033719539642333984\n",
      "0.0027055740356445312\n",
      "epoch 2 step 200 loss_fa 0.1921270489692688 loss_bp 0.029603250324726105\n",
      " time_fa 0.005678892135620117 time_bp 0.003430604934692383\n",
      "0.0022482872009277344\n",
      "epoch 2 step 300 loss_fa 0.17873330414295197 loss_bp 0.01415626797825098\n",
      " time_fa 0.005010843276977539 time_bp 0.003062725067138672\n",
      "0.0019481182098388672\n",
      "epoch 2 step 400 loss_fa 0.2301635593175888 loss_bp 0.06042161583900452\n",
      " time_fa 0.0058176517486572266 time_bp 0.0034215450286865234\n",
      "0.002396106719970703\n",
      "epoch 2 step 500 loss_fa 0.19925768673419952 loss_bp 0.018431803211569786\n",
      " time_fa 0.006294965744018555 time_bp 0.0040242671966552734\n",
      "0.0022706985473632812\n",
      "epoch 2 step 600 loss_fa 0.13576862215995789 loss_bp 0.009936237707734108\n",
      " time_fa 0.005123615264892578 time_bp 0.0034499168395996094\n",
      "0.0016736984252929688\n",
      "epoch 2 step 700 loss_fa 0.2684888541698456 loss_bp 0.032469287514686584\n",
      " time_fa 0.004915714263916016 time_bp 0.003137826919555664\n",
      "0.0017778873443603516\n",
      "epoch 2 step 800 loss_fa 0.14102204144001007 loss_bp 0.0573461577296257\n",
      " time_fa 0.004940509796142578 time_bp 0.0031011104583740234\n",
      "0.0018393993377685547\n",
      "epoch 2 step 900 loss_fa 0.11652084439992905 loss_bp 0.013993554748594761\n",
      " time_fa 0.004985809326171875 time_bp 0.003133058547973633\n",
      "0.0018527507781982422\n",
      "epoch 2 step 1000 loss_fa 0.3248976767063141 loss_bp 0.07720904052257538\n",
      " time_fa 0.005185604095458984 time_bp 0.0031442642211914062\n",
      "0.002041339874267578\n",
      "epoch 2 step 1100 loss_fa 0.07862537354230881 loss_bp 0.005635791923850775\n",
      " time_fa 0.005255222320556641 time_bp 0.0034551620483398438\n",
      "0.0018000602722167969\n",
      "epoch 2 step 1200 loss_fa 0.17898514866828918 loss_bp 0.041589751839637756\n",
      " time_fa 0.004848957061767578 time_bp 0.0031502246856689453\n",
      "0.0016987323760986328\n",
      "epoch 2 step 1300 loss_fa 0.2098841369152069 loss_bp 0.14915680885314941\n",
      " time_fa 0.005167722702026367 time_bp 0.003100156784057617\n",
      "0.00206756591796875\n",
      "epoch 2 step 1400 loss_fa 0.09477168321609497 loss_bp 0.03653829172253609\n",
      " time_fa 0.005109071731567383 time_bp 0.003099203109741211\n",
      "0.002009868621826172\n",
      "epoch 2 step 1500 loss_fa 0.2113684117794037 loss_bp 0.09187917411327362\n",
      " time_fa 0.00572967529296875 time_bp 0.0034859180450439453\n",
      "0.0022437572479248047\n",
      "epoch 2 step 1600 loss_fa 0.12924112379550934 loss_bp 0.03225640952587128\n",
      " time_fa 0.005066394805908203 time_bp 0.003122568130493164\n",
      "0.001943826675415039\n",
      "epoch 2 step 1700 loss_fa 0.14611630141735077 loss_bp 0.0755835548043251\n",
      " time_fa 0.005066394805908203 time_bp 0.003576517105102539\n",
      "0.001489877700805664\n",
      "epoch 2 step 1800 loss_fa 0.08319380134344101 loss_bp 0.006899493280798197\n",
      " time_fa 0.005030393600463867 time_bp 0.0034554004669189453\n",
      "0.0015749931335449219\n",
      "\n",
      "[Epoch 2] Test results\n",
      "\tFA: Avg. loss: -9.3153, Accuracy: 9628/10000 (96%)\n",
      "\tBP: Avg. loss: -10.9411, Accuracy: 9861/10000 (99%)\n",
      "\n",
      "epoch 3 step 100 loss_fa 0.06404955685138702 loss_bp 0.02452940307557583\n",
      " time_fa 0.004861593246459961 time_bp 0.0029265880584716797\n",
      "0.0019350051879882812\n",
      "epoch 3 step 200 loss_fa 0.04361429065465927 loss_bp 0.005729579366743565\n",
      " time_fa 0.005532503128051758 time_bp 0.003265380859375\n",
      "0.002267122268676758\n",
      "epoch 3 step 300 loss_fa 0.03751801326870918 loss_bp 0.006628094241023064\n",
      " time_fa 0.004937171936035156 time_bp 0.0029213428497314453\n",
      "0.002015829086303711\n",
      "epoch 3 step 400 loss_fa 0.15513773262500763 loss_bp 0.02141408994793892\n",
      " time_fa 0.004792928695678711 time_bp 0.003137350082397461\n",
      "0.00165557861328125\n",
      "epoch 3 step 500 loss_fa 0.17059944570064545 loss_bp 0.013567325659096241\n",
      " time_fa 0.005706787109375 time_bp 0.003266572952270508\n",
      "0.002440214157104492\n",
      "epoch 3 step 600 loss_fa 0.05723053216934204 loss_bp 0.012095061130821705\n",
      " time_fa 0.004811286926269531 time_bp 0.004168987274169922\n",
      "0.0006422996520996094\n",
      "epoch 3 step 700 loss_fa 0.27268749475479126 loss_bp 0.22794103622436523\n",
      " time_fa 0.011711359024047852 time_bp 0.003437042236328125\n",
      "0.008274316787719727\n",
      "epoch 3 step 800 loss_fa 0.08481299132108688 loss_bp 0.012788034975528717\n",
      " time_fa 0.011726140975952148 time_bp 0.003216266632080078\n",
      "0.00850987434387207\n",
      "epoch 3 step 900 loss_fa 0.3183770775794983 loss_bp 0.015383931808173656\n",
      " time_fa 0.0049326419830322266 time_bp 0.00393366813659668\n",
      "0.0009989738464355469\n",
      "epoch 3 step 1000 loss_fa 0.1066490113735199 loss_bp 0.012960804626345634\n",
      " time_fa 0.006103038787841797 time_bp 0.0034286975860595703\n",
      "0.0026743412017822266\n",
      "epoch 3 step 1100 loss_fa 0.33670806884765625 loss_bp 0.025301363319158554\n",
      " time_fa 0.005511760711669922 time_bp 0.004617452621459961\n",
      "0.0008943080902099609\n",
      "epoch 3 step 1200 loss_fa 0.1655379682779312 loss_bp 0.037661418318748474\n",
      " time_fa 0.006094932556152344 time_bp 0.0031125545501708984\n",
      "0.0029823780059814453\n",
      "epoch 3 step 1300 loss_fa 0.22325332462787628 loss_bp 0.03508083522319794\n",
      " time_fa 0.005511045455932617 time_bp 0.0031888484954833984\n",
      "0.0023221969604492188\n",
      "epoch 3 step 1400 loss_fa 0.0516044907271862 loss_bp 0.005011663306504488\n",
      " time_fa 0.005091667175292969 time_bp 0.0030934810638427734\n",
      "0.0019981861114501953\n",
      "epoch 3 step 1500 loss_fa 0.08211852610111237 loss_bp 0.11678095161914825\n",
      " time_fa 0.006217002868652344 time_bp 0.0033049583435058594\n",
      "0.0029120445251464844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 step 1600 loss_fa 0.040090594440698624 loss_bp 0.005274252500385046\n",
      " time_fa 0.004968404769897461 time_bp 0.003067493438720703\n",
      "0.0019009113311767578\n",
      "epoch 3 step 1700 loss_fa 0.1470952332019806 loss_bp 0.060219209641218185\n",
      " time_fa 0.005074501037597656 time_bp 0.004164218902587891\n",
      "0.0009102821350097656\n",
      "epoch 3 step 1800 loss_fa 0.18636389076709747 loss_bp 0.09798267483711243\n",
      " time_fa 0.006555795669555664 time_bp 0.003075838088989258\n",
      "0.0034799575805664062\n",
      "\n",
      "[Epoch 3] Test results\n",
      "\tFA: Avg. loss: -9.7200, Accuracy: 9676/10000 (97%)\n",
      "\tBP: Avg. loss: -11.4094, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "epoch 4 step 100 loss_fa 0.1837296485900879 loss_bp 0.05126965790987015\n",
      " time_fa 0.0068013668060302734 time_bp 0.0031311511993408203\n",
      "0.003670215606689453\n",
      "epoch 4 step 200 loss_fa 0.035294707864522934 loss_bp 0.0018402845598757267\n",
      " time_fa 0.004996776580810547 time_bp 0.0045490264892578125\n",
      "0.0004477500915527344\n",
      "epoch 4 step 300 loss_fa 0.08040939271450043 loss_bp 0.03319190815091133\n",
      " time_fa 0.005611896514892578 time_bp 0.0030698776245117188\n",
      "0.0025420188903808594\n",
      "epoch 4 step 400 loss_fa 0.16800211369991302 loss_bp 0.07455481588840485\n",
      " time_fa 0.0053746700286865234 time_bp 0.0029697418212890625\n",
      "0.002404928207397461\n",
      "epoch 4 step 500 loss_fa 0.05146801099181175 loss_bp 0.013114812783896923\n",
      " time_fa 0.00539851188659668 time_bp 0.0030546188354492188\n",
      "0.002343893051147461\n",
      "epoch 4 step 600 loss_fa 0.18448138236999512 loss_bp 0.08978169411420822\n",
      " time_fa 0.005070209503173828 time_bp 0.003369569778442383\n",
      "0.0017006397247314453\n",
      "epoch 4 step 700 loss_fa 0.07167518138885498 loss_bp 0.012213697656989098\n",
      " time_fa 0.01015019416809082 time_bp 0.0032308101654052734\n",
      "0.006919384002685547\n",
      "epoch 4 step 800 loss_fa 0.01656821183860302 loss_bp 0.005173672456294298\n",
      " time_fa 0.005079746246337891 time_bp 0.0034999847412109375\n",
      "0.0015797615051269531\n",
      "epoch 4 step 900 loss_fa 0.1940152496099472 loss_bp 0.04192733392119408\n",
      " time_fa 0.009931325912475586 time_bp 0.0031995773315429688\n",
      "0.006731748580932617\n",
      "epoch 4 step 1000 loss_fa 0.04391907528042793 loss_bp 0.013025695458054543\n",
      " time_fa 0.009847640991210938 time_bp 0.003321409225463867\n",
      "0.00652623176574707\n",
      "epoch 4 step 1100 loss_fa 0.011321702040731907 loss_bp 0.024963470175862312\n",
      " time_fa 0.01164698600769043 time_bp 0.0034284591674804688\n",
      "0.008218526840209961\n",
      "epoch 4 step 1200 loss_fa 0.17152784764766693 loss_bp 0.04757275804877281\n",
      " time_fa 0.005882740020751953 time_bp 0.003084421157836914\n",
      "0.002798318862915039\n",
      "epoch 4 step 1300 loss_fa 0.05879436805844307 loss_bp 0.01616404764354229\n",
      " time_fa 0.005076169967651367 time_bp 0.0031461715698242188\n",
      "0.0019299983978271484\n",
      "epoch 4 step 1400 loss_fa 0.06108479201793671 loss_bp 0.044708941131830215\n",
      " time_fa 0.005071401596069336 time_bp 0.0030808448791503906\n",
      "0.0019905567169189453\n",
      "epoch 4 step 1500 loss_fa 0.07855246216058731 loss_bp 0.032668136060237885\n",
      " time_fa 0.009003877639770508 time_bp 0.0032529830932617188\n",
      "0.005750894546508789\n",
      "epoch 4 step 1600 loss_fa 0.12928582727909088 loss_bp 0.0072778742760419846\n",
      " time_fa 0.004997730255126953 time_bp 0.003116607666015625\n",
      "0.0018811225891113281\n",
      "epoch 4 step 1700 loss_fa 0.1077403873205185 loss_bp 0.014916683547198772\n",
      " time_fa 0.004907369613647461 time_bp 0.003043651580810547\n",
      "0.001863718032836914\n",
      "epoch 4 step 1800 loss_fa 0.05771026015281677 loss_bp 0.02050592191517353\n",
      " time_fa 0.005213499069213867 time_bp 0.0032787322998046875\n",
      "0.0019347667694091797\n",
      "\n",
      "[Epoch 4] Test results\n",
      "\tFA: Avg. loss: -10.2581, Accuracy: 9696/10000 (97%)\n",
      "\tBP: Avg. loss: -12.1019, Accuracy: 9872/10000 (99%)\n",
      "\n",
      "epoch 5 step 100 loss_fa 0.10008199512958527 loss_bp 0.028494594618678093\n",
      " time_fa 0.009730339050292969 time_bp 0.003050088882446289\n",
      "0.00668025016784668\n",
      "epoch 5 step 200 loss_fa 0.01870994083583355 loss_bp 0.0023964340798556805\n",
      " time_fa 0.005139827728271484 time_bp 0.0032491683959960938\n",
      "0.0018906593322753906\n",
      "epoch 5 step 300 loss_fa 0.0692770853638649 loss_bp 0.028373004868626595\n",
      " time_fa 0.004971981048583984 time_bp 0.0043697357177734375\n",
      "0.0006022453308105469\n",
      "epoch 5 step 400 loss_fa 0.04744839295744896 loss_bp 0.006043943110853434\n",
      " time_fa 0.00506281852722168 time_bp 0.0030355453491210938\n",
      "0.002027273178100586\n",
      "epoch 5 step 500 loss_fa 0.09731867909431458 loss_bp 0.00991469994187355\n",
      " time_fa 0.00521087646484375 time_bp 0.0032968521118164062\n",
      "0.0019140243530273438\n",
      "epoch 5 step 600 loss_fa 0.05234648287296295 loss_bp 0.021615251898765564\n",
      " time_fa 0.006229400634765625 time_bp 0.003202199935913086\n",
      "0.003027200698852539\n",
      "epoch 5 step 700 loss_fa 0.12162435054779053 loss_bp 0.01915309950709343\n",
      " time_fa 0.004931211471557617 time_bp 0.0030460357666015625\n",
      "0.0018851757049560547\n",
      "epoch 5 step 800 loss_fa 0.182474285364151 loss_bp 0.006012806203216314\n",
      " time_fa 0.011382102966308594 time_bp 0.0030562877655029297\n",
      "0.008325815200805664\n",
      "epoch 5 step 900 loss_fa 0.13195054233074188 loss_bp 0.046550851315259933\n",
      " time_fa 0.0049741268157958984 time_bp 0.003417491912841797\n",
      "0.0015566349029541016\n",
      "epoch 5 step 1000 loss_fa 0.11473606526851654 loss_bp 0.010167188942432404\n",
      " time_fa 0.004670381546020508 time_bp 0.002949953079223633\n",
      "0.001720428466796875\n",
      "epoch 5 step 1100 loss_fa 0.02570297382771969 loss_bp 0.001769807655364275\n",
      " time_fa 0.0051000118255615234 time_bp 0.0033578872680664062\n",
      "0.0017421245574951172\n",
      "epoch 5 step 1200 loss_fa 0.04200688749551773 loss_bp 0.00570353027433157\n",
      " time_fa 0.009830236434936523 time_bp 0.003247976303100586\n",
      "0.0065822601318359375\n",
      "epoch 5 step 1300 loss_fa 0.04513562470674515 loss_bp 0.006358277052640915\n",
      " time_fa 0.009892940521240234 time_bp 0.004522085189819336\n",
      "0.0053708553314208984\n",
      "epoch 5 step 1400 loss_fa 0.09066536277532578 loss_bp 0.010640612803399563\n",
      " time_fa 0.005578041076660156 time_bp 0.0043773651123046875\n",
      "0.0012006759643554688\n",
      "epoch 5 step 1500 loss_fa 0.016496194526553154 loss_bp 0.0011010580928996205\n",
      " time_fa 0.005090236663818359 time_bp 0.0031175613403320312\n",
      "0.001972675323486328\n",
      "epoch 5 step 1600 loss_fa 0.20516742765903473 loss_bp 0.05161987245082855\n",
      " time_fa 0.004983425140380859 time_bp 0.0031151771545410156\n",
      "0.0018682479858398438\n",
      "epoch 5 step 1700 loss_fa 0.02065022848546505 loss_bp 0.005092706996947527\n",
      " time_fa 0.0046498775482177734 time_bp 0.0030188560485839844\n",
      "0.001631021499633789\n",
      "epoch 5 step 1800 loss_fa 0.0598212331533432 loss_bp 0.0018634588923305273\n",
      " time_fa 0.00987386703491211 time_bp 0.0034339427947998047\n",
      "0.006439924240112305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8c491a6d1501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mt_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mt_avg_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_fa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/biotorch/.venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/biotorch/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/projects/biotorch/.venv/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/biotorch/biotorch/autograd/functions.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(context, grad_output)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             grad_kernels = torch.nn.grad.conv2d_weight(input=input,\n\u001b[0m\u001b[1;32m     74\u001b[0m                                                        \u001b[0mweight_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernels_fa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                                        \u001b[0mgrad_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/biotorch/.venv/lib/python3.8/site-packages/torch/nn/grad.py\u001b[0m in \u001b[0;36mconv2d_weight\u001b[0;34m(input, weight_size, grad_output, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                     input.shape[2], input.shape[3])\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     grad_weight = torch.conv2d(input, grad_output, None, dilation, padding,\n\u001b[0m\u001b[1;32m    210\u001b[0m                                stride, in_channels * min_batch)\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
      "epoch 0 step 100 loss_fa 0.12437755614519119 loss_bp 0.008606676012277603\n",
      " time_fa 0.008363008499145508 time_bp 0.006242036819458008\n",
      "0.0021209716796875\n",
      "epoch 0 step 200 loss_fa 0.21762007474899292 loss_bp 0.05049574002623558\n",
      " time_fa 0.009228944778442383 time_bp 0.006242036819458008\n",
      "0.002986907958984375\n",
      "epoch 0 step 300 loss_fa 0.24455222487449646 loss_bp 0.077779121696949\n",
      " time_fa 0.008696556091308594 time_bp 0.006389141082763672\n",
      "0.002307415008544922\n",
      "epoch 0 step 400 loss_fa 0.32020407915115356 loss_bp 0.18049514293670654\n",
      " time_fa 0.009351491928100586 time_bp 0.005378246307373047\n",
      "0.003973245620727539\n",
      "epoch 0 step 500 loss_fa 0.15975359082221985 loss_bp 0.06126094236969948\n",
      " time_fa 0.008693695068359375 time_bp 0.00639033317565918\n",
      "0.0023033618927001953\n",
      "epoch 0 step 600 loss_fa 0.22966253757476807 loss_bp 0.007768119685351849\n",
      " time_fa 0.009076356887817383 time_bp 0.00702977180480957\n",
      "0.0020465850830078125\n",
      "epoch 0 step 700 loss_fa 0.2555553913116455 loss_bp 0.02789762057363987\n",
      " time_fa 0.010265111923217773 time_bp 0.007197856903076172\n",
      "0.0030672550201416016\n",
      "epoch 0 step 800 loss_fa 0.15583786368370056 loss_bp 0.02998223528265953\n",
      " time_fa 0.009760618209838867 time_bp 0.0057964324951171875\n",
      "0.00396418571472168\n",
      "epoch 0 step 900 loss_fa 0.29179614782333374 loss_bp 0.031301677227020264\n",
      " time_fa 0.009650707244873047 time_bp 0.005705833435058594\n",
      "0.003944873809814453\n",
      "epoch 0 step 1000 loss_fa 0.11898261308670044 loss_bp 0.008558633737266064\n",
      " time_fa 0.01439046859741211 time_bp 0.010759592056274414\n",
      "0.0036308765411376953\n",
      "epoch 0 step 1100 loss_fa 0.19769538938999176 loss_bp 0.08962711691856384\n",
      " time_fa 0.010110616683959961 time_bp 0.006778717041015625\n",
      "0.003331899642944336\n",
      "epoch 0 step 1200 loss_fa 0.2341926246881485 loss_bp 0.1501055210828781\n",
      " time_fa 0.010025262832641602 time_bp 0.008581161499023438\n",
      "0.001444101333618164\n",
      "epoch 0 step 1300 loss_fa 0.07444526255130768 loss_bp 0.041576534509658813\n",
      " time_fa 0.009635448455810547 time_bp 0.006325483322143555\n",
      "0.003309965133666992\n",
      "epoch 0 step 1400 loss_fa 0.05547015741467476 loss_bp 0.006100814323872328\n",
      " time_fa 0.008727073669433594 time_bp 0.006613969802856445\n",
      "0.0021131038665771484\n",
      "epoch 0 step 1500 loss_fa 0.1910737156867981 loss_bp 0.010443746112287045\n",
      " time_fa 0.009766578674316406 time_bp 0.006820201873779297\n",
      "0.0029463768005371094\n",
      "epoch 0 step 1600 loss_fa 0.4142473340034485 loss_bp 0.05581042915582657\n",
      " time_fa 0.009557962417602539 time_bp 0.0055980682373046875\n",
      "0.0039598941802978516\n",
      "epoch 0 step 1700 loss_fa 0.06347371637821198 loss_bp 0.008311057463288307\n",
      " time_fa 0.009968757629394531 time_bp 0.006694793701171875\n",
      "0.0032739639282226562\n",
      "epoch 0 step 1800 loss_fa 0.08518880605697632 loss_bp 0.03439881652593613\n",
      " time_fa 0.009177684783935547 time_bp 0.007356405258178711\n",
      "0.001821279525756836\n",
      "\n",
      "[Epoch 0] Test results\n",
      "\tFA: Avg. loss: -8.7296, Accuracy: 9503/10000 (95%)\n",
      "\tBP: Avg. loss: -10.5164, Accuracy: 9848/10000 (98%)\n",
      "\n"
>>>>>>> 89e987570592dabbe351e6b7ae691d14bdcd116c
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for idx_batch, (inputs, targets) in enumerate(train_loader):\n",
    "        # flatten the inputs from square image to 1d vector\n",
    "        #inputs = inputs.view(batch_size, -1)\n",
    "        # wrap them into varaibles\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        # get outputs from the model\n",
    "        #print(\"inputs = \", inputs.size())\n",
    "        outputs_fa = model_fa(inputs)\n",
    "        outputs_bp = model_bp(inputs)\n",
    "        # print(outputs_fa.size())\n",
    "        # print(outputs_bp.size())\n",
    "        # calculate loss\n",
    "        outputs_fa = torch.squeeze(outputs_fa)\n",
    "        outputs_bp = torch.squeeze(outputs_bp)\n",
    "        # print(outputs_fa.size())\n",
    "        # print(outputs_bp.size())\n",
    "\n",
    "        # print(\"-\"*20)\n",
    "        #print(\"targets.size() = \", targets.size())\n",
    "        # input()\n",
    "        \n",
    "        loss_bp = loss_crossentropy(outputs_bp, targets)\n",
    "        loss_fa = loss_crossentropy(outputs_fa, targets)\n",
    "        # print(loss_bp, loss_fa)\n",
    "        \n",
    "        t_fa = time.time()\n",
    "        model_fa.zero_grad()\n",
    "        loss_fa.backward()\n",
    "        optimizer_fa.step()\n",
    "        t_avg_fa = time.time() - t_fa\n",
    "    \n",
    "        t_bp = time.time()\n",
    "        model_bp.zero_grad()\n",
    "        loss_bp.backward()\n",
    "        optimizer_bp.step()\n",
    "        t_avg_bp = time.time() - t_bp\n",
    "\n",
    "        if (idx_batch + 1) % 100 == 0:\n",
    "            train_log = 'epoch ' + str(epoch) + ' step ' + str(idx_batch + 1) + \\\n",
    "                        ' loss_fa ' + str(loss_fa.data.item()) + ' loss_bp ' + str(loss_bp.data.item())\n",
    "                         \n",
    "            times = ' time_fa '+ str(t_avg_fa) + ' time_bp ' + str(t_avg_bp)\n",
    "            time_dif = t_avg_fa - t_avg_bp\n",
    "            print(train_log)\n",
    "            print(times)\n",
    "            print(time_dif)\n",
    "            logger_train.write(train_log + '\\n')\n",
    "\n",
    "    # Test models\n",
    "    test_loss_fa, correct_fa = test(model_fa, test_loader, batch_size)    \n",
    "    test_loss_bp, correct_bp = test(model_bp, test_loader, batch_size)\n",
    "\n",
    "    print('\\n[Epoch {}] Test results'.format(epoch))\n",
    "    print('\\tFA: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss_fa,\n",
    "                                                                      correct_fa, len(test_loader.dataset), 100. * correct_fa / len(test_loader.dataset)))\n",
    "    print('\\tBP: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss_bp,\n",
    "                                                                        correct_bp, len(test_loader.dataset), 100. * correct_bp / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-camera",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
